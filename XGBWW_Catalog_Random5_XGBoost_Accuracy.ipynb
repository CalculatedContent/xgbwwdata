{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/CalculatedContent/xgbwwdata/blob/main/XGBWW_Catalog_Random5_XGBoost_Accuracy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# XGBWW catalog-driven random-per-source XGBoost benchmark\n",
        "\n",
        "This notebook:\n",
        "1. Loads the catalog DataFrame produced by `XGBWW_Dataset_Catalog_Checkpoint.ipynb`.\n",
        "2. Randomly samples **5 datasets per source** (classification only).\n",
        "3. Trains one XGBoost model per sampled dataset with a small CV-based round selection.\n",
        "4. Reports train and test accuracies per dataset and aggregated by source.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Mount Google Drive and configure paths\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "\n",
        "# ===== USER CONFIG =====\n",
        "CATALOG_CSV = Path(\"/content/drive/MyDrive/xgbwwdata/catalog_checkpoint/dataset_catalog.csv\")\n",
        "RANDOM_SEED = 42\n",
        "SAMPLES_PER_SOURCE = 5\n",
        "TEST_SIZE = 0.20\n",
        "# =======================\n",
        "\n",
        "drive.mount(\"/content/drive\")\n",
        "print(\"Catalog path:\", CATALOG_CSV)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Install dependencies\n",
        "\n",
        "Use the same repository-install flow as the other Colab notebooks (no `pip install xgbwwdata`).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Install xgbwwdata from a fresh clone using the repository installer script\n",
        "!rm -rf /content/repo_xgbwwdata\n",
        "!git clone https://github.com/CalculatedContent/xgbwwdata.git /content/repo_xgbwwdata\n",
        "%run /content/repo_xgbwwdata/scripts/colab_install.py --repo /content/repo_xgbwwdata\n",
        "\n",
        "# Notebook-specific dependencies\n",
        "%pip install -q openml pmlb keel-ds xgboost scikit-learn\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from xgbwwdata import Filters, load_dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Load catalog and pick 5 random datasets per source\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "if not CATALOG_CSV.exists():\n",
        "    raise FileNotFoundError(f\"Catalog not found: {CATALOG_CSV}. Run XGBWW_Dataset_Catalog_Checkpoint.ipynb first.\")\n",
        "\n",
        "df_catalog = pd.read_csv(CATALOG_CSV)\n",
        "print(\"Catalog shape:\", df_catalog.shape)\n",
        "\n",
        "required_cols = {\"dataset_uid\", \"source\", \"task_type\"}\n",
        "missing = required_cols - set(df_catalog.columns)\n",
        "if missing:\n",
        "    raise ValueError(f\"Catalog is missing required columns: {missing}\")\n",
        "\n",
        "# Accuracy is for classification; keep classification-like tasks\n",
        "df_cls = df_catalog[df_catalog[\"task_type\"].astype(str).str.contains(\"classification\", case=False, na=False)].copy()\n",
        "if df_cls.empty:\n",
        "    raise ValueError(\"No classification datasets found in catalog.\")\n",
        "\n",
        "# Sample up to SAMPLES_PER_SOURCE per source\n",
        "def sample_per_source(group):\n",
        "    n = min(SAMPLES_PER_SOURCE, len(group))\n",
        "    return group.sample(n=n, random_state=RANDOM_SEED)\n",
        "\n",
        "df_pick = (\n",
        "    df_cls.groupby(\"source\", group_keys=False)\n",
        "    .apply(sample_per_source)\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "print(\"Selected datasets:\", len(df_pick))\n",
        "display(df_pick[[\"source\", \"dataset_uid\", \"name\", \"task_type\"]].sort_values([\"source\", \"dataset_uid\"]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Train one XGBoost model per sampled dataset and report accuracies\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "filters = Filters(\n",
        "    min_rows=200,\n",
        "    max_rows=60000,\n",
        "    max_features=50000,\n",
        "    max_dense_elements=int(2e8),\n",
        ")\n",
        "\n",
        "\n",
        "def fit_and_score(dataset_uid: str, source: str):\n",
        "    X, y, meta = load_dataset(dataset_uid, filters=filters)\n",
        "\n",
        "    y = np.asarray(y)\n",
        "    classes, y_enc = np.unique(y, return_inverse=True)\n",
        "    n_classes = len(classes)\n",
        "    if n_classes < 2:\n",
        "        raise ValueError(f\"Dataset {dataset_uid} has <2 classes after loading.\")\n",
        "\n",
        "    stratify = y_enc if n_classes > 1 else None\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y_enc, test_size=TEST_SIZE, random_state=RANDOM_SEED, stratify=stratify\n",
        "    )\n",
        "\n",
        "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "    dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "\n",
        "    if n_classes == 2:\n",
        "        params = {\n",
        "            \"objective\": \"binary:logistic\",\n",
        "            \"eval_metric\": \"logloss\",\n",
        "            \"tree_method\": \"hist\",\n",
        "            \"learning_rate\": 0.05,\n",
        "            \"max_depth\": 6,\n",
        "            \"subsample\": 0.85,\n",
        "            \"colsample_bytree\": 0.85,\n",
        "            \"min_child_weight\": 2.0,\n",
        "            \"reg_lambda\": 2.0,\n",
        "            \"seed\": RANDOM_SEED,\n",
        "        }\n",
        "        cv = xgb.cv(\n",
        "            params=params,\n",
        "            dtrain=dtrain,\n",
        "            num_boost_round=1200,\n",
        "            nfold=5,\n",
        "            stratified=True,\n",
        "            early_stopping_rounds=50,\n",
        "            seed=RANDOM_SEED,\n",
        "            verbose_eval=False,\n",
        "        )\n",
        "        rounds = len(cv)\n",
        "        model = xgb.train(params=params, dtrain=dtrain, num_boost_round=rounds, verbose_eval=False)\n",
        "\n",
        "        yhat_tr = (model.predict(dtrain) >= 0.5).astype(int)\n",
        "        yhat_te = (model.predict(dtest) >= 0.5).astype(int)\n",
        "    else:\n",
        "        params = {\n",
        "            \"objective\": \"multi:softprob\",\n",
        "            \"num_class\": n_classes,\n",
        "            \"eval_metric\": \"mlogloss\",\n",
        "            \"tree_method\": \"hist\",\n",
        "            \"learning_rate\": 0.05,\n",
        "            \"max_depth\": 7,\n",
        "            \"subsample\": 0.9,\n",
        "            \"colsample_bytree\": 0.9,\n",
        "            \"min_child_weight\": 1.0,\n",
        "            \"reg_lambda\": 1.0,\n",
        "            \"seed\": RANDOM_SEED,\n",
        "        }\n",
        "        cv = xgb.cv(\n",
        "            params=params,\n",
        "            dtrain=dtrain,\n",
        "            num_boost_round=1200,\n",
        "            nfold=5,\n",
        "            stratified=True,\n",
        "            early_stopping_rounds=60,\n",
        "            seed=RANDOM_SEED,\n",
        "            verbose_eval=False,\n",
        "        )\n",
        "        rounds = len(cv)\n",
        "        model = xgb.train(params=params, dtrain=dtrain, num_boost_round=rounds, verbose_eval=False)\n",
        "\n",
        "        yhat_tr = np.argmax(model.predict(dtrain), axis=1)\n",
        "        yhat_te = np.argmax(model.predict(dtest), axis=1)\n",
        "\n",
        "    return {\n",
        "        \"source\": source,\n",
        "        \"dataset_uid\": dataset_uid,\n",
        "        \"dataset_name\": meta.get(\"name\"),\n",
        "        \"n_rows\": int(meta.get(\"n_rows\", len(y))),\n",
        "        \"n_features\": int(meta.get(\"n_features\", X.shape[1] if hasattr(X, \"shape\") else -1)),\n",
        "        \"n_classes\": int(n_classes),\n",
        "        \"rounds\": int(rounds),\n",
        "        \"train_accuracy\": float(accuracy_score(y_train, yhat_tr)),\n",
        "        \"test_accuracy\": float(accuracy_score(y_test, yhat_te)),\n",
        "    }\n",
        "\n",
        "\n",
        "results = []\n",
        "errors = []\n",
        "\n",
        "for row in df_pick.itertuples(index=False):\n",
        "    uid = row.dataset_uid\n",
        "    source = row.source\n",
        "    print(f\"Training: {uid}\")\n",
        "    try:\n",
        "        results.append(fit_and_score(uid, source))\n",
        "    except Exception as e:\n",
        "        errors.append({\"source\": source, \"dataset_uid\": uid, \"error\": str(e)})\n",
        "        print(f\"  Skipped {uid}: {e}\")\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "errors_df = pd.DataFrame(errors)\n",
        "\n",
        "print(\"\\nCompleted:\", len(results_df), \"datasets\")\n",
        "if not errors_df.empty:\n",
        "    print(\"Errors:\", len(errors_df))\n",
        "    display(errors_df.head(20))\n",
        "\n",
        "display(results_df.sort_values([\"source\", \"test_accuracy\"], ascending=[True, False]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Summary tables (train/test accuracies)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "if results_df.empty:\n",
        "    print(\"No successful trainings.\")\n",
        "else:\n",
        "    summary = (\n",
        "        results_df.groupby(\"source\", as_index=False)[[\"train_accuracy\", \"test_accuracy\"]]\n",
        "        .agg([\"mean\", \"std\", \"min\", \"max\"])\n",
        "    )\n",
        "    summary.columns = [\"source\"] + [f\"{a}_{b}\" for a, b in summary.columns.tolist()[1:]]\n",
        "\n",
        "    print(\"Per-dataset results:\")\n",
        "    display(results_df.sort_values([\"source\", \"test_accuracy\"], ascending=[True, False]))\n",
        "\n",
        "    print(\"\\nPer-source summary:\")\n",
        "    display(summary.sort_values(\"test_accuracy_mean\", ascending=False))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "XGBWW_Catalog_Random5_XGBoost_Accuracy.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}