{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# xgbwwdata Checkpoint Validation Notebook (per-source first model)\n",
    "\n",
    "This notebook validates that a previously saved multisource checkpoint can be reloaded and reused.\n",
    "It selects the **first model per data source** from the checkpoint, trains **one additional XGBoost model** per selected dataset, and evaluates holdout accuracy/log loss.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Imports and runtime settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "\n",
    "from xgbwwdata import Filters, load_dataset\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "RNG = 0\n",
    "TEST_SIZE = 0.20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Load the checkpoint created by the multisource experiment\n",
    "\n",
    "By default this looks for files saved by `XGBWW_Multisource_Experiment.ipynb` under Google Drive.\n",
    "If needed, set `CHECKPOINT_PATH` manually to a specific feather file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional manual override:\n",
    "CHECKPOINT_PATH = None\n",
    "\n",
    "GDRIVE_DIR = \"/content/drive/MyDrive/xgboost2ww_runs\"\n",
    "PATTERN = os.path.join(GDRIVE_DIR, \"*_multisource_results_*.feather\")\n",
    "\n",
    "if CHECKPOINT_PATH is None:\n",
    "    files = sorted(glob.glob(PATTERN))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No checkpoint files found under: {PATTERN}\")\n",
    "    CHECKPOINT_PATH = files[-1]\n",
    "\n",
    "print(\"Using checkpoint:\", CHECKPOINT_PATH)\n",
    "\n",
    "df_ckpt = pd.read_feather(CHECKPOINT_PATH)\n",
    "print(\"Checkpoint rows:\", len(df_ckpt), \"| columns:\", len(df_ckpt.columns))\n",
    "display(df_ckpt.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Pick the first model from each source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "required_cols = {\"dataset_uid\", \"source\", \"rounds\"}\n",
    "missing = required_cols - set(df_ckpt.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"Checkpoint is missing required columns: {missing}\")\n",
    "\n",
    "# Preserve original row order from the checkpoint, then pick first row per source.\n",
    "df_ordered = df_ckpt.reset_index(drop=False).rename(columns={\"index\": \"checkpoint_row\"})\n",
    "df_first_per_source = (\n",
    "    df_ordered.sort_values(\"checkpoint_row\")\n",
    "    .groupby(\"source\", as_index=False)\n",
    "    .first()\n",
    "    .sort_values(\"source\")\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(\"Sources in checkpoint:\", df_ckpt[\"source\"].nunique())\n",
    "print(\"Rows selected (first per source):\", len(df_first_per_source))\n",
    "display(df_first_per_source[[c for c in [\"source\",\"dataset\",\"dataset_uid\",\"rounds\",\"good_test_acc\"] if c in df_first_per_source.columns]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Reload each selected dataset, train one extra boost model, and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = Filters(\n",
    "    min_rows=200,\n",
    "    max_rows=60000,\n",
    "    max_features=50_000,\n",
    "    max_dense_elements=int(2e8),\n",
    "    preprocess=True,\n",
    ")\n",
    "\n",
    "\n",
    "def train_extra_model_for_row(rec: pd.Series) -> dict:\n",
    "    dataset_uid = rec[\"dataset_uid\"]\n",
    "    source = rec[\"source\"]\n",
    "\n",
    "    X, y, meta = load_dataset(dataset_uid, filters=filters)\n",
    "    y = np.asarray(y)\n",
    "\n",
    "    if len(np.unique(y)) != 2:\n",
    "        raise ValueError(f\"Dataset is not binary classification: {dataset_uid}\")\n",
    "\n",
    "    idx = np.arange(len(y))\n",
    "    tr_idx, te_idx = train_test_split(idx, test_size=TEST_SIZE, random_state=RNG, stratify=y)\n",
    "    Xtr, Xte = X[tr_idx], X[te_idx]\n",
    "    ytr, yte = y[tr_idx], y[te_idx]\n",
    "\n",
    "    if hasattr(Xtr, \"tocsr\"):\n",
    "        Xtr = Xtr.tocsr().astype(np.float32)\n",
    "        Xte = Xte.tocsr().astype(np.float32)\n",
    "    else:\n",
    "        Xtr = np.asarray(Xtr, dtype=np.float32)\n",
    "        Xte = np.asarray(Xte, dtype=np.float32)\n",
    "\n",
    "    rounds = int(rec.get(\"rounds\", 100))\n",
    "    extra_rounds = max(rounds + 25, 50)\n",
    "\n",
    "    params = dict(\n",
    "        objective=\"binary:logistic\",\n",
    "        eval_metric=\"logloss\",\n",
    "        tree_method=\"hist\",\n",
    "        seed=RNG,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=5,\n",
    "        min_child_weight=3.0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_lambda=1.0,\n",
    "    )\n",
    "\n",
    "    dtr = xgb.DMatrix(Xtr, label=ytr)\n",
    "    dte = xgb.DMatrix(Xte, label=yte)\n",
    "    bst = xgb.train(params=params, dtrain=dtr, num_boost_round=extra_rounds, verbose_eval=False)\n",
    "\n",
    "    p_te = bst.predict(dte).astype(np.float32)\n",
    "    yhat = (p_te >= 0.5).astype(int)\n",
    "\n",
    "    return dict(\n",
    "        source=source,\n",
    "        dataset_uid=dataset_uid,\n",
    "        dataset=meta.get(\"name\", rec.get(\"dataset\", dataset_uid)),\n",
    "        checkpoint_row=int(rec[\"checkpoint_row\"]),\n",
    "        checkpoint_rounds=rounds,\n",
    "        extra_rounds=extra_rounds,\n",
    "        checkpoint_good_test_acc=float(rec.get(\"good_test_acc\", np.nan)),\n",
    "        extra_test_acc=float(accuracy_score(yte, yhat)),\n",
    "        extra_test_logloss=float(log_loss(yte, p_te, labels=[0, 1])),\n",
    "        n_rows=int(X.shape[0]),\n",
    "        n_features=int(X.shape[1]),\n",
    "    )\n",
    "\n",
    "\n",
    "rows = []\n",
    "start = time.time()\n",
    "for _, rec in df_first_per_source.iterrows():\n",
    "    out = train_extra_model_for_row(rec)\n",
    "    rows.append(out)\n",
    "    print(\n",
    "        f\"[{len(rows)}/{len(df_first_per_source)}] {out['source']}: {out['dataset']} | \"\n",
    "        f\"checkpoint_acc={out['checkpoint_good_test_acc']:.3f} | extra_acc={out['extra_test_acc']:.3f}\",\n",
    "        flush=True,\n",
    "    )\n",
    "\n",
    "\n",
    "df_validation = pd.DataFrame(rows).sort_values(\"source\").reset_index(drop=True)\n",
    "print(f\"Done in {(time.time()-start)/60:.1f} min.\")\n",
    "display(df_validation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Sanity checks and optional save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df_validation) == df_ckpt[\"source\"].nunique(),     \"Expected exactly one validation row per source.\"\n",
    "\n",
    "print(\"Per-source checkpoint validation complete.\")\n",
    "print(\"Mean extra test accuracy:\", float(df_validation[\"extra_test_acc\"].mean()))\n",
    "\n",
    "# Optional output path\n",
    "OUT_CSV = None  # e.g., \"/content/drive/MyDrive/xgboost2ww_runs/checkpoint_validation.csv\"\n",
    "if OUT_CSV:\n",
    "    df_validation.to_csv(OUT_CSV, index=False)\n",
    "    print(\"Saved:\", OUT_CSV)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}