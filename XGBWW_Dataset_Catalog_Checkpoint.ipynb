{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "<a href=\"https://colab.research.google.com/github/CalculatedContent/xgbwwdata/blob/main/XGBWW_Dataset_Catalog_Checkpoint.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# xgbwwdata Dataset Catalog Builder (with Drive checkpoint + resume)\n\nThis notebook scans one or many `xgbwwdata` sources, builds a **dataset catalog DataFrame**, and stores it in Google Drive as a checkpoint.\n\nThe catalog includes per-dataset metadata such as:\n- source + source dataset ID\n- unique identifier\n- dataset name\n- number of rows/features/classes\n- experiment type (regression, binary classification, multiclass classification, single-class)\n\nIt supports:\n- **Test mode** (e.g., only 2â€“3 datasets per source)\n- **Full mode** (scan all datasets)\n- **Resume mode** (continue from previous checkpoint without restarting from scratch)\n- **Source selection** (all sources or a specific subset)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1) Mount Google Drive and configure paths/options\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "from google.colab import drive\nfrom pathlib import Path\n\n# ====== USER CONFIG ======\n# If None, scan all supported sources: openml, pmlb, keel, amlb, libsvm\nTARGET_SOURCES = None                  # e.g. [\"openml\", \"pmlb\"]\n\nTEST_MODE = True\nTEST_PER_SOURCE = 3                    # used only when TEST_MODE=True\n\nSMOKE_TRAIN = False                    # True = try 1-round XGBoost train validation\nSAVE_EVERY = 10                        # checkpoint save frequency (processed datasets)\n\n# Structural filters (same spirit as xgbwwdata defaults)\nMIN_ROWS = 200\nMAX_ROWS = 60000\nMAX_FEATURES = 50000\nMAX_DENSE_ELEMENTS = int(2e8)\n\n# Drive output folder\nDRIVE_BASE = Path(\"/content/drive/MyDrive/xgbwwdata/catalog_checkpoint\")\nCATALOG_CSV = DRIVE_BASE / \"dataset_catalog.csv\"\nPROGRESS_JSON = DRIVE_BASE / \"scan_progress.json\"\n# ==========================\n\ndrive.mount('/content/drive')\nDRIVE_BASE.mkdir(parents=True, exist_ok=True)\nprint(f\"Catalog CSV: {CATALOG_CSV}\")\nprint(f\"Progress JSON: {PROGRESS_JSON}\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2) Install dependencies\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Install xgbwwdata from a fresh clone using the Colab installer script\\n",
        "!rm -rf /content/repo_xgbwwdata\\n",
        "!git clone https://github.com/CalculatedContent/xgbwwdata.git /content/repo_xgbwwdata\\n",
        "%run /content/repo_xgbwwdata/scripts/colab_install.py --repo /content/repo_xgbwwdata\\n",
        "\\n",
        "# Notebook-specific dependencies\\n",
        "%pip install -q openml pmlb keel-ds xgboost tqdm pyarrow\\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3) Imports and helpers\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import json\nimport time\nfrom datetime import datetime, timezone\nfrom typing import Dict, Iterable, List, Optional\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\n\nfrom xgbwwdata import Filters, enable_logging\nfrom xgbwwdata.registry import LibSVMIndex, _libsvm_load, _smoke_train_1round\nfrom xgbwwdata.sources.openml import OpenMLSource\nfrom xgbwwdata.sources.pmlb import PMLBSource\nfrom xgbwwdata.sources.keel import KEELSource\nfrom xgbwwdata.sources.amlb import AMLBSource\n\nenable_logging()\n\n\ndef utc_now_iso() -> str:\n    return datetime.now(timezone.utc).isoformat()\n\n\ndef infer_experiment_type(task_type: str, n_classes) -> str:\n    if task_type == \"regression\":\n        return \"regression\"\n    if pd.isna(n_classes):\n        return \"classification_unknown\"\n    n_classes = int(n_classes)\n    if n_classes <= 1:\n        return \"single_class\"\n    if n_classes == 2:\n        return \"binary_classification\"\n    return \"multiclass_classification\"\n\n\ndef source_factories() -> Dict[str, callable]:\n    return {\n        \"openml\": lambda: OpenMLSource(),\n        \"pmlb\": lambda: PMLBSource(include_regression=True),\n        \"keel\": lambda: KEELSource(),\n        \"amlb\": lambda: AMLBSource(),\n        \"libsvm\": lambda: None,\n    }\n\n\ndef normalize_sources(target_sources: Optional[List[str]]) -> List[str]:\n    all_sources = list(source_factories().keys())\n    if target_sources is None:\n        return all_sources\n    cleaned = [s.lower().strip() for s in target_sources]\n    bad = [s for s in cleaned if s not in all_sources]\n    if bad:\n        raise ValueError(f\"Unknown sources: {bad}. Allowed: {all_sources}\")\n    return cleaned\n\n\ndef load_checkpoint(catalog_csv, progress_json):\n    if catalog_csv.exists():\n        catalog_df = pd.read_csv(catalog_csv)\n    else:\n        catalog_df = pd.DataFrame(columns=[\n            \"source\",\n            \"source_dataset_id\",\n            \"dataset_uid\",\n            \"unique_id\",\n            \"name\",\n            \"task_type\",\n            \"experiment_type\",\n            \"n_rows\",\n            \"n_features\",\n            \"n_classes\",\n            \"scan_timestamp_utc\",\n        ])\n\n    if progress_json.exists():\n        with open(progress_json, \"r\") as f:\n            progress = json.load(f)\n    else:\n        progress = {\n            \"processed\": {},   # dataset_uid -> {status, updated_at, error?}\n            \"last_save_utc\": None,\n        }\n\n    # Backfill from existing CSV if needed\n    if \"dataset_uid\" in catalog_df.columns:\n        for uid in catalog_df[\"dataset_uid\"].dropna().astype(str):\n            progress[\"processed\"].setdefault(uid, {\"status\": \"ok\", \"updated_at\": utc_now_iso()})\n\n    return catalog_df, progress\n\n\ndef save_checkpoint(catalog_df, progress, catalog_csv, progress_json):\n    catalog_df = catalog_df.drop_duplicates(subset=[\"dataset_uid\"], keep=\"last\")\n    catalog_df = catalog_df.sort_values([\"source\", \"dataset_uid\"]).reset_index(drop=True)\n    catalog_df.to_csv(catalog_csv, index=False)\n    progress[\"last_save_utc\"] = utc_now_iso()\n    with open(progress_json, \"w\") as f:\n        json.dump(progress, f, indent=2)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4) Run scan with checkpoint/resume support\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "filters = Filters(\n    min_rows=MIN_ROWS,\n    max_rows=MAX_ROWS,\n    max_features=MAX_FEATURES,\n    max_dense_elements=MAX_DENSE_ELEMENTS,\n)\n\nselected_sources = normalize_sources(TARGET_SOURCES)\ncatalog_df, progress = load_checkpoint(CATALOG_CSV, PROGRESS_JSON)\nprocessed = progress[\"processed\"]\n\nprint(\"Selected sources:\", selected_sources)\nprint(\"Existing catalog rows:\", len(catalog_df))\nprint(\"Existing processed entries:\", len(processed))\n\nnew_rows = []\nprocessed_since_save = 0\n\nfor source_name in selected_sources:\n    print(f\"\n=== Scanning source: {source_name} ===\")\n\n    # Build source iterator\n    if source_name == \"libsvm\":\n        lib_idx = LibSVMIndex()\n        uid_iter = list(lib_idx.iter_uids())\n        src_obj = None\n    else:\n        src_obj = source_factories()[source_name]()\n        uid_iter = list(src_obj.iter_ids())\n\n    # Test mode: keep just first N not-yet-processed from this source\n    if TEST_MODE:\n        remaining = [uid for uid in uid_iter if uid not in processed][:TEST_PER_SOURCE]\n        uid_iter = remaining\n        print(f\"Test mode ON -> scanning up to {len(uid_iter)} dataset(s) for {source_name}\")\n    else:\n        print(f\"Full mode -> candidate dataset count: {len(uid_iter)}\")\n\n    for uid in tqdm(uid_iter, desc=f\"{source_name} datasets\"):\n        if uid in processed:\n            continue\n\n        try:\n            if source_name == \"libsvm\":\n                X, y, meta = _libsvm_load(uid, filters)\n                task_type = \"classification\"\n                n_classes = int(meta.get(\"n_classes\", np.nan)) if not pd.isna(meta.get(\"n_classes\", np.nan)) else np.nan\n                name = str(meta.get(\"name\", uid.split(\":\", 1)[1]))\n                n_rows = int(meta.get(\"n_rows\", X.shape[0]))\n                n_features = int(meta.get(\"n_features\", X.shape[1]))\n\n                if SMOKE_TRAIN:\n                    if not _smoke_train_1round(X, y, task_type, n_classes if not pd.isna(n_classes) else None, seed=0):\n                        raise RuntimeError(\"smoke_train_failed\")\n            else:\n                ok, info, X, y, task_type, n_classes, name = src_obj.validate_and_prepare(uid, filters)\n                if not ok:\n                    raise RuntimeError(f\"filtered_out:{info}\")\n\n                if SMOKE_TRAIN:\n                    if not _smoke_train_1round(X, y, task_type, n_classes, seed=0):\n                        raise RuntimeError(\"smoke_train_failed\")\n\n                info = info if isinstance(info, dict) else {}\n                n_rows = int(info.get(\"n_rows\", X.shape[0]))\n                n_features = int(info.get(\"n_features\", X.shape[1]))\n\n            src, src_id = uid.split(\":\", 1)\n            experiment_type = infer_experiment_type(task_type, n_classes)\n\n            row = {\n                \"source\": src,\n                \"source_dataset_id\": src_id,\n                \"dataset_uid\": uid,\n                \"unique_id\": f\"{src}|{src_id}\",\n                \"name\": name,\n                \"task_type\": task_type,\n                \"experiment_type\": experiment_type,\n                \"n_rows\": n_rows,\n                \"n_features\": n_features,\n                \"n_classes\": n_classes,\n                \"scan_timestamp_utc\": utc_now_iso(),\n            }\n            new_rows.append(row)\n\n            processed[uid] = {\n                \"status\": \"ok\",\n                \"source\": src,\n                \"updated_at\": utc_now_iso(),\n            }\n\n        except Exception as e:\n            processed[uid] = {\n                \"status\": \"error\",\n                \"source\": source_name,\n                \"error\": str(e),\n                \"updated_at\": utc_now_iso(),\n            }\n\n        processed_since_save += 1\n        if processed_since_save >= SAVE_EVERY:\n            if new_rows:\n                catalog_df = pd.concat([catalog_df, pd.DataFrame(new_rows)], ignore_index=True)\n                new_rows = []\n            save_checkpoint(catalog_df, progress, CATALOG_CSV, PROGRESS_JSON)\n            processed_since_save = 0\n            print(f\"Checkpoint saved at {utc_now_iso()} | catalog rows={len(catalog_df)}\")\n\n# Final save\nif new_rows:\n    catalog_df = pd.concat([catalog_df, pd.DataFrame(new_rows)], ignore_index=True)\n\nsave_checkpoint(catalog_df, progress, CATALOG_CSV, PROGRESS_JSON)\nprint(\"\nScan complete.\")\nprint(\"Catalog rows:\", len(catalog_df.drop_duplicates(subset=['dataset_uid'])))\nprint(\"Checkpoint files updated.\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5) Inspect the resulting dataset catalog\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "catalog_df = pd.read_csv(CATALOG_CSV)\nprint(\"Catalog shape:\", catalog_df.shape)\n\n# Useful sorted preview\npreview_cols = [\n    \"source\", \"source_dataset_id\", \"dataset_uid\", \"name\",\n    \"task_type\", \"experiment_type\", \"n_rows\", \"n_features\", \"n_classes\"\n]\n\ncatalog_df = catalog_df.sort_values([\"source\", \"n_rows\"], ascending=[True, False]).reset_index(drop=True)\ndisplay(catalog_df[preview_cols].head(30))\n\n# Summary by source and experiment type\ndisplay(\n    catalog_df.groupby([\"source\", \"experiment_type\"], dropna=False)\n    .size()\n    .reset_index(name=\"dataset_count\")\n    .sort_values([\"source\", \"dataset_count\"], ascending=[True, False])\n)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 6) Notes\n\n- Re-running this notebook continues from the checkpoint in Drive.\n- To scan only one source, set `TARGET_SOURCES = [\"openml\"]` (or another source).\n- To run full scan, set `TEST_MODE = False`.\n- The saved checkpoint CSV is your reusable dataset registry for future experiments.\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
