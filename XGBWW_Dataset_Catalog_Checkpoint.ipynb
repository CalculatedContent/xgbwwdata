{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VHk0MugHzpKb"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/CalculatedContent/xgbwwdata/blob/main/XGBWW_Dataset_Catalog_Checkpoint.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
   ],
   "id": "VHk0MugHzpKb"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uwac8XxqzpKd"
   },
   "source": [
    "# xgbwwdata Dataset Catalog Builder (with Drive checkpoint + resume)\n",
    "\n",
    "This notebook scans one or many `xgbwwdata` sources, builds a **dataset catalog DataFrame**, and stores it in Google Drive as a checkpoint.\n",
    "\n",
    "The catalog includes per-dataset metadata such as:\n",
    "- source + source dataset ID\n",
    "- unique identifier\n",
    "- dataset name\n",
    "- number of rows/features/classes\n",
    "- experiment type (regression, binary classification, multiclass classification, single-class)\n",
    "\n",
    "It supports:\n",
    "- **Test mode** (e.g., only 2\u20133 datasets per source)\n",
    "- **Full mode** (scan all datasets)\n",
    "- **Resume mode** (continue from previous checkpoint without restarting from scratch)\n",
    "- **Source selection** (all sources or a specific subset)\n"
   ],
   "id": "uwac8XxqzpKd"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w5c6PWmnzpKd"
   },
   "source": [
    "## 1) Mount Google Drive and configure paths/options\n"
   ],
   "id": "w5c6PWmnzpKd"
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Hu23L14zpKe",
    "outputId": "9b065bf3-d685-42a4-e438-97de9b760330"
   },
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "Catalog CSV: /content/drive/MyDrive/xgbwwdata/catalog_checkpoint/dataset_catalog.csv\n",
      "Progress JSON: /content/drive/MyDrive/xgbwwdata/catalog_checkpoint/scan_progress.json\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "from pathlib import Path\n",
    "\n",
    "# ====== USER CONFIG ======\n",
    "# If None, scan all supported sources: openml, pmlb, keel, amlb, libsvm\n",
    "TARGET_SOURCES = None                  # e.g. [\"openml\", \"pmlb\"]\n",
    "\n",
    "TEST_MODE = False\n",
    "TEST_PER_SOURCE = 3                    # used only when TEST_MODE=True\n",
    "\n",
    "SMOKE_TRAIN = False                    # True = try 1-round XGBoost train validation\n",
    "SAVE_EVERY = 10                        # checkpoint save frequency (processed datasets)\n",
    "\n",
    "# Structural filters (same spirit as xgbwwdata defaults)\n",
    "MIN_ROWS = 200\n",
    "MAX_ROWS = 60000\n",
    "MAX_FEATURES = 50000\n",
    "MAX_DENSE_ELEMENTS = int(2e8)\n",
    "\n",
    "# Drive output folder\n",
    "DRIVE_BASE = Path(\"/content/drive/MyDrive/xgbwwdata/catalog_checkpoint\")\n",
    "CATALOG_CSV = DRIVE_BASE / \"dataset_catalog.csv\"\n",
    "PROGRESS_JSON = DRIVE_BASE / \"scan_progress.json\"\n",
    "# ==========================\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "DRIVE_BASE.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Catalog CSV: {CATALOG_CSV}\")\n",
    "print(f\"Progress JSON: {PROGRESS_JSON}\")\n"
   ],
   "id": "6Hu23L14zpKe"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "83hABbArzpKe"
   },
   "source": [
    "## 2) Install dependencies\n"
   ],
   "id": "83hABbArzpKe"
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUwWo1K7zpKe",
    "outputId": "a72b9722-c66b-48d8-b0f2-b74192465e8f"
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cloning into '/content/repo_xgbwwdata'...\n",
      "remote: Enumerating objects: 113, done.\u001b[K\n",
      "remote: Counting objects: 100% (13/13), done.\u001b[K\n",
      "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
      "remote: Total 113 (delta 0), reused 0 (delta 0), pack-reused 100 (from 2)\u001b[K\n",
      "Receiving objects: 100% (113/113), 71.42 KiB | 6.49 MiB/s, done.\n",
      "Resolving deltas: 100% (37/37), done.\n",
      "+ /usr/bin/python3 -m pip install -U pip setuptools wheel\n",
      "+ /usr/bin/python3 -m pip install -r /content/repo_xgbwwdata/requirements.txt\n",
      "+ /usr/bin/python3 -m pip install -e /content/repo_xgbwwdata --no-build-isolation --no-deps\n",
      "module: <module 'xgbwwdata' from '/content/repo_xgbwwdata/src/xgbwwdata/__init__.py'>\n",
      "__file__: /content/repo_xgbwwdata/src/xgbwwdata/__init__.py\n",
      "__path__: ['/content/repo_xgbwwdata/src/xgbwwdata']\n",
      "exports: ['Filters', 'ScanOptions', 'config', 'enable_logging', 'load_dataset', 'logging', 'registry', 'scan_datasets', 'sources']\n",
      "import OK\n"
     ]
    }
   ],
   "source": [
    "# Install xgbwwdata from a fresh clone using the Colab installer script\n",
    "!rm -rf /content/repo_xgbwwdata\n",
    "!git clone https://github.com/CalculatedContent/xgbwwdata.git /content/repo_xgbwwdata\n",
    "%run /content/repo_xgbwwdata/scripts/colab_install.py --repo /content/repo_xgbwwdata\n",
    "# Notebook-specific dependencies\n",
    "%pip install -q openml pmlb keel-ds xgboost tqdm pyarrow"
   ],
   "id": "EUwWo1K7zpKe"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G_E4rNvmzpKf"
   },
   "source": [
    "## 3) Imports and helpers\n"
   ],
   "id": "G_E4rNvmzpKf"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "L6k_6BddzpKf"
   },
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import gc\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime, timezone\n",
    "from typing import Dict, Iterable, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from xgbwwdata import Filters, enable_logging\n",
    "from xgbwwdata.registry import LibSVMIndex, _libsvm_load, _smoke_train_1round\n",
    "from xgbwwdata.sources.openml import OpenMLSource\n",
    "from xgbwwdata.sources.pmlb import PMLBSource\n",
    "from xgbwwdata.sources.keel import KEELSource\n",
    "from xgbwwdata.sources.amlb import AMLBSource\n",
    "\n",
    "enable_logging()\n",
    "\n",
    "\n",
    "def utc_now_iso() -> str:\n",
    "    return datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "\n",
    "def infer_experiment_type(task_type: str, n_classes) -> str:\n",
    "    if task_type == \"regression\":\n",
    "        return \"regression\"\n",
    "    if pd.isna(n_classes):\n",
    "        return \"classification_unknown\"\n",
    "    n_classes = int(n_classes)\n",
    "    if n_classes <= 1:\n",
    "        return \"single_class\"\n",
    "    if n_classes == 2:\n",
    "        return \"binary_classification\"\n",
    "    return \"multiclass_classification\"\n",
    "\n",
    "\n",
    "def source_factories() -> Dict[str, callable]:\n",
    "    return {\n",
    "        \"openml\": lambda: OpenMLSource(),\n",
    "        \"pmlb\": lambda: PMLBSource(include_regression=True),\n",
    "        \"keel\": lambda: KEELSource(),\n",
    "        \"amlb\": lambda: AMLBSource(),\n",
    "        \"libsvm\": lambda: None,\n",
    "    }\n",
    "\n",
    "\n",
    "def normalize_sources(target_sources: Optional[List[str]]) -> List[str]:\n",
    "    all_sources = list(source_factories().keys())\n",
    "    if target_sources is None:\n",
    "        return all_sources\n",
    "    cleaned = [s.lower().strip() for s in target_sources]\n",
    "    bad = [s for s in cleaned if s not in all_sources]\n",
    "    if bad:\n",
    "        raise ValueError(f\"Unknown sources: {bad}. Allowed: {all_sources}\")\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "def load_checkpoint(catalog_csv, progress_json):\n",
    "    if catalog_csv.exists():\n",
    "        catalog_df = pd.read_csv(catalog_csv)\n",
    "    else:\n",
    "        catalog_df = pd.DataFrame(columns=[\n",
    "            \"source\",\n",
    "            \"source_dataset_id\",\n",
    "            \"dataset_uid\",\n",
    "            \"unique_id\",\n",
    "            \"name\",\n",
    "            \"task_type\",\n",
    "            \"experiment_type\",\n",
    "            \"n_rows\",\n",
    "            \"n_features\",\n",
    "            \"n_classes\",\n",
    "            \"scan_timestamp_utc\",\n",
    "        ])\n",
    "\n",
    "    if progress_json.exists():\n",
    "        with open(progress_json, \"r\") as f:\n",
    "            progress = json.load(f)\n",
    "    else:\n",
    "        progress = {\n",
    "            \"processed\": {},   # dataset_uid -> {status, updated_at, error?}\n",
    "            \"last_save_utc\": None,\n",
    "        }\n",
    "\n",
    "    # Backfill from existing CSV if needed\n",
    "    if \"dataset_uid\" in catalog_df.columns:\n",
    "        for uid in catalog_df[\"dataset_uid\"].dropna().astype(str):\n",
    "            progress[\"processed\"].setdefault(uid, {\"status\": \"ok\", \"updated_at\": utc_now_iso()})\n",
    "\n",
    "    return catalog_df, progress\n",
    "\n",
    "\n",
    "def save_checkpoint(catalog_df, progress, catalog_csv, progress_json):\n",
    "    catalog_df = catalog_df.drop_duplicates(subset=[\"dataset_uid\"], keep=\"last\")\n",
    "    catalog_df = catalog_df.sort_values([\"source\", \"dataset_uid\"]).reset_index(drop=True)\n",
    "    catalog_df.to_csv(catalog_csv, index=False)\n",
    "    progress[\"last_save_utc\"] = utc_now_iso()\n",
    "    with open(progress_json, \"w\") as f:\n",
    "        json.dump(progress, f, indent=2)\n"
   ],
   "id": "L6k_6BddzpKf"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xl4Kqe8kzpKf"
   },
   "source": [
    "## 4) Run scan with checkpoint/resume support\n"
   ],
   "id": "Xl4Kqe8kzpKf"
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208,
     "referenced_widgets": [
      "76d5999832c941ea903ce7b6ee504aa7",
      "7f5d93a960d443e5a6312187393c0fa9",
      "0e02bf1620414c46914a357642d6d6f6",
      "ce97a61247d041e389980f9cbd6eee5a",
      "ee69a986b65e46ceb6c6bd3a11a5eaf1",
      "0933e2ae8d3a447f9533cef06daff1cd",
      "42df4671b8514c059d9fe54c1c1c19b8",
      "9b2169efdd884e57a0a3d9bf81653bbc",
      "4aa1ce71557f4d849e8685b59e1bf73b",
      "ca0bfb13ddef43828865450e8f74a3d6",
      "9c01987406734224b727656b5dd032be"
     ]
    },
    "id": "2JWs3fMlzpKf",
    "outputId": "69b6c9bf-1072-49a0-8463-986d9352df51"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Selected sources: ['openml', 'pmlb', 'keel', 'amlb', 'libsvm']\n",
      "Existing catalog rows: 174\n",
      "Existing processed entries: 210\n",
      " Scanning source: openml\n",
      "Full mode -> candidate dataset count: 6383\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "openml datasets:   0%|          | 0/6383 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "76d5999832c941ea903ce7b6ee504aa7"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/openml/datasets/dataset.py:472: FutureWarning: factorize with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  pd.factorize(type_)[0]\n",
      "WARNING:openml.datasets.functions:Could not download file from https://data.openml.org/datasets/0000/0274/dataset_274.pq: Object at 'https://data.openml.org/datasets/0000/0274/dataset_274.pq' does not exist.\n"
     ]
    }
   ],
   "source": [
    "filters = Filters(\n",
    "    min_rows=MIN_ROWS,\n",
    "    max_rows=MAX_ROWS,\n",
    "    max_features=MAX_FEATURES,\n",
    "    max_dense_elements=MAX_DENSE_ELEMENTS,\n",
    ")\n",
    "\n",
    "selected_sources = normalize_sources(TARGET_SOURCES)\n",
    "catalog_df, progress = load_checkpoint(CATALOG_CSV, PROGRESS_JSON)\n",
    "processed = progress[\"processed\"]\n",
    "\n",
    "print(\"Selected sources:\", selected_sources)\n",
    "print(\"Existing catalog rows:\", len(catalog_df))\n",
    "print(\"Existing processed entries:\", len(processed))\n",
    "\n",
    "new_rows = []\n",
    "processed_since_save = 0\n",
    "\n",
    "for source_name in selected_sources:\n",
    "    print(f\" Scanning source: {source_name}\")\n",
    "\n",
    "    # Build source iterator\n",
    "    if source_name == \"libsvm\":\n",
    "        lib_idx = LibSVMIndex()\n",
    "        uid_iter = list(lib_idx.iter_uids())\n",
    "        src_obj = None\n",
    "    elif source_name == \"openml\":\n",
    "        src_obj = source_factories()[source_name]()\n",
    "        # OpenML can be very large; pre-filter with metadata if available to avoid OOM/runtime crashes.\n",
    "        try:\n",
    "            meta_df = src_obj._openml.datasets.list_datasets(output_format=\"dataframe\")\n",
    "            did_col = \"did\" if \"did\" in meta_df.columns else (\"dataset_id\" if \"dataset_id\" in meta_df.columns else None)\n",
    "            rows_col = \"NumberOfInstances\" if \"NumberOfInstances\" in meta_df.columns else None\n",
    "            feat_col = \"NumberOfFeatures\" if \"NumberOfFeatures\" in meta_df.columns else None\n",
    "\n",
    "            if did_col is None:\n",
    "                uid_iter = []\n",
    "            else:\n",
    "                filtered = meta_df.copy()\n",
    "                if rows_col is not None:\n",
    "                    filtered = filtered[(filtered[rows_col].isna()) | (\n",
    "                        (filtered[rows_col] >= MIN_ROWS) & (filtered[rows_col] <= MAX_ROWS)\n",
    "                    )]\n",
    "                if feat_col is not None:\n",
    "                    filtered = filtered[(filtered[feat_col].isna()) | (filtered[feat_col] <= MAX_FEATURES)]\n",
    "\n",
    "                uid_iter = [f\"openml:{int(did)}\" for did in filtered[did_col].dropna().astype(int).tolist()]\n",
    "                print(\n",
    "                    f\"OpenML metadata pre-filter active: {len(uid_iter)} candidates \"\n",
    "                    f\"(from {len(meta_df)} total)\"\n",
    "                )\n",
    "        except Exception as e:\n",
    "            print(f\"OpenML metadata pre-filter unavailable ({e}); falling back to full ID list.\")\n",
    "            uid_iter = list(src_obj.iter_ids())\n",
    "    else:\n",
    "        src_obj = source_factories()[source_name]()\n",
    "        uid_iter = list(src_obj.iter_ids())\n",
    "\n",
    "    # Test mode: keep just first N not-yet-processed from this source\n",
    "    if TEST_MODE:\n",
    "        remaining = [uid for uid in uid_iter if uid not in processed][:TEST_PER_SOURCE]\n",
    "        uid_iter = remaining\n",
    "        print(f\"Test mode ON -> scanning up to {len(uid_iter)} dataset(s) for {source_name}\")\n",
    "    else:\n",
    "        print(f\"Full mode -> candidate dataset count: {len(uid_iter)}\")\n",
    "\n",
    "    for uid in tqdm(uid_iter, desc=f\"{source_name} datasets\"):\n",
    "        if uid in processed:\n",
    "            continue\n",
    "\n",
    "        X = y = None\n",
    "        try:\n",
    "            if source_name == \"libsvm\":\n",
    "                X, y, meta = _libsvm_load(uid, filters)\n",
    "                task_type = \"classification\"\n",
    "                n_classes = int(meta.get(\"n_classes\", np.nan)) if not pd.isna(meta.get(\"n_classes\", np.nan)) else np.nan\n",
    "                name = str(meta.get(\"name\", uid.split(\":\", 1)[1]))\n",
    "                n_rows = int(meta.get(\"n_rows\", X.shape[0]))\n",
    "                n_features = int(meta.get(\"n_features\", X.shape[1]))\n",
    "\n",
    "                if SMOKE_TRAIN:\n",
    "                    if not _smoke_train_1round(X, y, task_type, n_classes if not pd.isna(n_classes) else None, seed=0):\n",
    "                        raise RuntimeError(\"smoke_train_failed\")\n",
    "            else:\n",
    "                ok, info, X, y, task_type, n_classes, name = src_obj.validate_and_prepare(uid, filters)\n",
    "                if not ok:\n",
    "                    raise RuntimeError(f\"filtered_out:{info}\")\n",
    "\n",
    "                if SMOKE_TRAIN:\n",
    "                    if not _smoke_train_1round(X, y, task_type, n_classes, seed=0):\n",
    "                        raise RuntimeError(\"smoke_train_failed\")\n",
    "\n",
    "                info = info if isinstance(info, dict) else {}\n",
    "                n_rows = int(info.get(\"n_rows\", X.shape[0]))\n",
    "                n_features = int(info.get(\"n_features\", X.shape[1]))\n",
    "\n",
    "            src, src_id = uid.split(\":\", 1)\n",
    "            experiment_type = infer_experiment_type(task_type, n_classes)\n",
    "\n",
    "            row = {\n",
    "                \"source\": src,\n",
    "                \"source_dataset_id\": src_id,\n",
    "                \"dataset_uid\": uid,\n",
    "                \"unique_id\": f\"{src}|{src_id}\",\n",
    "                \"name\": name,\n",
    "                \"task_type\": task_type,\n",
    "                \"experiment_type\": experiment_type,\n",
    "                \"n_rows\": n_rows,\n",
    "                \"n_features\": n_features,\n",
    "                \"n_classes\": n_classes,\n",
    "                \"scan_timestamp_utc\": utc_now_iso(),\n",
    "            }\n",
    "            new_rows.append(row)\n",
    "\n",
    "            processed[uid] = {\n",
    "                \"status\": \"ok\",\n",
    "                \"source\": src,\n",
    "                \"updated_at\": utc_now_iso(),\n",
    "            }\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except BaseException as e:\n",
    "            # Catch broad failures (network/remote corruption/rare loader errors) and continue.\n",
    "            processed[uid] = {\n",
    "                \"status\": \"error\",\n",
    "                \"source\": source_name,\n",
    "                \"error\": str(e),\n",
    "                \"updated_at\": utc_now_iso(),\n",
    "            }\n",
    "        finally:\n",
    "            # Proactively release memory to keep long Colab sessions stable.\n",
    "            del X, y\n",
    "            gc.collect()\n",
    "\n",
    "        processed_since_save += 1\n",
    "        if processed_since_save >= SAVE_EVERY:\n",
    "            if new_rows:\n",
    "                catalog_df = pd.concat([catalog_df, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "                new_rows = []\n",
    "            save_checkpoint(catalog_df, progress, CATALOG_CSV, PROGRESS_JSON)\n",
    "            processed_since_save = 0\n",
    "            print(f\"Checkpoint saved at {utc_now_iso()} | catalog rows={len(catalog_df)}\")\n",
    "\n",
    "# Final save\n",
    "if new_rows:\n",
    "    catalog_df = pd.concat([catalog_df, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "\n",
    "save_checkpoint(catalog_df, progress, CATALOG_CSV, PROGRESS_JSON)\n",
    "print(\"Scan complete.\")\n",
    "print(\"Catalog rows:\", len(catalog_df.drop_duplicates(subset=['dataset_uid'])))\n",
    "print(\"Checkpoint files updated.\")\n"
   ],
   "id": "2JWs3fMlzpKf"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_YvKg6DrzpKg"
   },
   "source": [
    "## 5) Inspect the resulting dataset catalog\n"
   ],
   "id": "_YvKg6DrzpKg"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "j2UexlU9zpKg"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "catalog_df = pd.read_csv(CATALOG_CSV)\n",
    "print(\"Catalog shape:\", catalog_df.shape)\n",
    "\n",
    "# Useful sorted preview\n",
    "preview_cols = [\n",
    "    \"source\", \"source_dataset_id\", \"dataset_uid\", \"name\",\n",
    "    \"task_type\", \"experiment_type\", \"n_rows\", \"n_features\", \"n_classes\"\n",
    "]\n",
    "\n",
    "catalog_df = catalog_df.sort_values([\"source\", \"n_rows\"], ascending=[True, False]).reset_index(drop=True)\n",
    "display(catalog_df[preview_cols].head(30))\n",
    "\n",
    "# Summary by source and experiment type\n",
    "display(\n",
    "    catalog_df.groupby([\"source\", \"experiment_type\"], dropna=False)\n",
    "    .size()\n",
    "    .reset_index(name=\"dataset_count\")\n",
    "    .sort_values([\"source\", \"dataset_count\"], ascending=[True, False])\n",
    ")\n"
   ],
   "id": "j2UexlU9zpKg"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r_Ml2KRpzpKg"
   },
   "source": [
    "## 6) Notes\n",
    "\n",
    "- Re-running this notebook continues from the checkpoint in Drive.\n",
    "- To scan only one source, set `TARGET_SOURCES = [\"openml\"]` (or another source).\n",
    "- To run full scan, set `TEST_MODE = False`.\n",
    "- The saved checkpoint CSV is your reusable dataset registry for future experiments.\n"
   ],
   "id": "r_Ml2KRpzpKg"
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "rTH--nmXZIuT"
   },
   "id": "rTH--nmXZIuT",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  },
  "colab": {
   "provenance": [],
   "machine_shape": "hm"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "76d5999832c941ea903ce7b6ee504aa7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7f5d93a960d443e5a6312187393c0fa9",
       "IPY_MODEL_0e02bf1620414c46914a357642d6d6f6",
       "IPY_MODEL_ce97a61247d041e389980f9cbd6eee5a"
      ],
      "layout": "IPY_MODEL_ee69a986b65e46ceb6c6bd3a11a5eaf1"
     }
    },
    "7f5d93a960d443e5a6312187393c0fa9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0933e2ae8d3a447f9533cef06daff1cd",
      "placeholder": "\u200b",
      "style": "IPY_MODEL_42df4671b8514c059d9fe54c1c1c19b8",
      "value": "openml\u2007datasets:\u2007\u2007\u20073%"
     }
    },
    "0e02bf1620414c46914a357642d6d6f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9b2169efdd884e57a0a3d9bf81653bbc",
      "max": 6383,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4aa1ce71557f4d849e8685b59e1bf73b",
      "value": 194
     }
    },
    "ce97a61247d041e389980f9cbd6eee5a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ca0bfb13ddef43828865450e8f74a3d6",
      "placeholder": "\u200b",
      "style": "IPY_MODEL_9c01987406734224b727656b5dd032be",
      "value": "\u2007194/6383\u2007[00:21&lt;16:08,\u2007\u20076.39it/s]"
     }
    },
    "ee69a986b65e46ceb6c6bd3a11a5eaf1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0933e2ae8d3a447f9533cef06daff1cd": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "42df4671b8514c059d9fe54c1c1c19b8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9b2169efdd884e57a0a3d9bf81653bbc": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4aa1ce71557f4d849e8685b59e1bf73b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ca0bfb13ddef43828865450e8f74a3d6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9c01987406734224b727656b5dd032be": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}